{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture4-cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1kim5gzbV1J",
        "colab_type": "text"
      },
      "source": [
        "# 4. Convolutional networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mioLfdKbV1L",
        "colab_type": "text"
      },
      "source": [
        "- Used as part of INFO8010 Deep Learning (Gilles Louppe, 2018-2019).\n",
        "- Originally adapted from [Pytorch tutorial for Deep Learning researchers](https://github.com/yunjey/pytorch-tutorial) (Yunvey Choi, 2018).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3_srXC9bV1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtbXFK9qbV1V",
        "colab_type": "text"
      },
      "source": [
        "# Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOmVVeJZbV1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CeAhEJ2bV1b",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b0A-Cu0bV1c",
        "colab_type": "code",
        "outputId": "a6a41fa8-69b3-44cf-e951-248aa99183d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# MNIST Dataset (Images and Labels)\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3901209.68it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 134267.69it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2016527.30it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 51294.98it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQNaLHCxbV1h",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1cNFS25bV1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding = 2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding = 2),# CONV,\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = CNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaK8iUmlbV1n",
        "colab_type": "text"
      },
      "source": [
        "# Loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkre49L-bV1o",
        "colab_type": "code",
        "outputId": "4d09c9bf-4373-44e6-e2f5-337de8e77667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "# Set parameters to be updated.\n",
        "\n",
        "\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):        \n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
        "                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/5], Step: [100/600], Loss: 0.1497\n",
            "Epoch: [1/5], Step: [200/600], Loss: 0.1004\n",
            "Epoch: [1/5], Step: [300/600], Loss: 0.1092\n",
            "Epoch: [1/5], Step: [400/600], Loss: 0.0211\n",
            "Epoch: [1/5], Step: [500/600], Loss: 0.0444\n",
            "Epoch: [1/5], Step: [600/600], Loss: 0.0167\n",
            "Epoch: [2/5], Step: [100/600], Loss: 0.0268\n",
            "Epoch: [2/5], Step: [200/600], Loss: 0.0402\n",
            "Epoch: [2/5], Step: [300/600], Loss: 0.0333\n",
            "Epoch: [2/5], Step: [400/600], Loss: 0.0672\n",
            "Epoch: [2/5], Step: [500/600], Loss: 0.0463\n",
            "Epoch: [2/5], Step: [600/600], Loss: 0.0567\n",
            "Epoch: [3/5], Step: [100/600], Loss: 0.0214\n",
            "Epoch: [3/5], Step: [200/600], Loss: 0.0300\n",
            "Epoch: [3/5], Step: [300/600], Loss: 0.0086\n",
            "Epoch: [3/5], Step: [400/600], Loss: 0.0242\n",
            "Epoch: [3/5], Step: [500/600], Loss: 0.0334\n",
            "Epoch: [3/5], Step: [600/600], Loss: 0.0077\n",
            "Epoch: [4/5], Step: [100/600], Loss: 0.0266\n",
            "Epoch: [4/5], Step: [200/600], Loss: 0.0096\n",
            "Epoch: [4/5], Step: [300/600], Loss: 0.0134\n",
            "Epoch: [4/5], Step: [400/600], Loss: 0.0298\n",
            "Epoch: [4/5], Step: [500/600], Loss: 0.0087\n",
            "Epoch: [4/5], Step: [600/600], Loss: 0.0214\n",
            "Epoch: [5/5], Step: [100/600], Loss: 0.0330\n",
            "Epoch: [5/5], Step: [200/600], Loss: 0.0295\n",
            "Epoch: [5/5], Step: [300/600], Loss: 0.0206\n",
            "Epoch: [5/5], Step: [400/600], Loss: 0.0712\n",
            "Epoch: [5/5], Step: [500/600], Loss: 0.0150\n",
            "Epoch: [5/5], Step: [600/600], Loss: 0.0259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgawClmabV1y",
        "colab_type": "text"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdY0YJzfbV1z",
        "colab_type": "code",
        "outputId": "b1de58e3-807d-4341-d4be-4a0265d7071f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test the Model\n",
        "model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for images, labels in test_loader:\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "    \n",
        "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the 10000 test images: 98 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nEWDHmubV16",
        "colab_type": "text"
      },
      "source": [
        "# Using a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImIIJ0aKbV18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load pretrained vgg16.\n",
        "import torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72rjS9Q0bV2F",
        "colab_type": "text"
      },
      "source": [
        "We load vgg16 a CNN trained on imagenet. \n",
        "![VGG16:](http://zike.io/upload_img/cnn/vgg-16-receptive-field.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvav-H9dbV2H",
        "colab_type": "code",
        "outputId": "5d34fb21-17ea-4fae-bd45-5eb4f9fd95d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "vgg16 = torchvision.models.vgg16(pretrained=True).eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 553433881/553433881 [00:05<00:00, 109123027.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghyc5xCgbV2M",
        "colab_type": "text"
      },
      "source": [
        "## Simple \"real life\" example with a pre-trained neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3wrHbDlbV2O",
        "colab_type": "text"
      },
      "source": [
        "We download a mapping between the class label as an integer into text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "e4mzyJhCbV2P",
        "colab_type": "code",
        "outputId": "928cc477-d660-4d79-e278-344ef30aede2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
        "import json\n",
        "class_idx = json.load(open(\"imagenet_class_index.json\"))\n",
        "idx2label = np.array([class_idx[str(k)][1] for k in range(len(class_idx))])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-15 07:22:45--  https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.19.35\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.19.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35363 (35K) [application/octet-stream]\n",
            "Saving to: ‘imagenet_class_index.json’\n",
            "\n",
            "\r          imagenet_   0%[                    ]       0  --.-KB/s               \rimagenet_class_inde 100%[===================>]  34.53K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-07-15 07:22:46 (429 KB/s) - ‘imagenet_class_index.json’ saved [35363/35363]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJS4_i4rbV2V",
        "colab_type": "text"
      },
      "source": [
        "vgg16 was trained on imagenet dataset which inputs have been normalized as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTWNrEdXbV2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
        "                                                                            std=(0.229, 0.224, 0.225))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUqF11HybV2a",
        "colab_type": "code",
        "outputId": "fb21bf76-6616-44f9-9dbf-88ffefc82629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "response = requests.get('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Indian-Ring-Necked-Parakeet-300x300.jpg')\n",
        "im = Image.open(BytesIO(response.content))\n",
        "im = transform(im)\n",
        "print(im.shape)\n",
        "probas = nn.Softmax()(vgg16(im.unsqueeze(0))).view(-1)\n",
        "sorted_ouputs = torch.argsort(probas, descending=True)\n",
        "for i in range(5):\n",
        "    print(\"Class label: {:s} - Probability: {:4f}\".format(\n",
        "        idx2label[sorted_ouputs[i]], probas[sorted_ouputs[i]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 300, 300])\n",
            "Class label: lorikeet - Probability: 0.505242\n",
            "Class label: bee_eater - Probability: 0.186730\n",
            "Class label: macaw - Probability: 0.113632\n",
            "Class label: toucan - Probability: 0.033876\n",
            "Class label: fig - Probability: 0.015141\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiJrHjnXbV2e",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning of pre-trained neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2r5mxVGbV2f",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE</b>:\n",
        "\n",
        "Investigate the output that is given by the network.\n",
        "Is it interpretable in terms of probabilities? If not, transform it in a way that matches what is done in [deep learning](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3JN7Yo_bV26",
        "colab_type": "text"
      },
      "source": [
        "Download CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWnd7f51bV28",
        "colab_type": "code",
        "outputId": "92148942-2fa5-4fda-db3e-58bb47dff4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# MNIST Dataset (Images and Labels)\n",
        "train_dataset = dsets.CIFAR10(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.CIFAR10(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 27688949.79it/s]                               \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llunO94FbV3D",
        "colab_type": "code",
        "outputId": "58026724-1006-4562-d7f0-b49e89f62c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "iterator = iter(train_loader)    \n",
        "samples, labels = iterator.next()\n",
        "\n",
        "print(vgg16.avgpool(vgg16.features(samples)).shape)\n",
        "# Replace top layer for finetuning, e.g. for CIFAR10.\n",
        "vgg16.classifier = nn.Linear(25088, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 512, 7, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpxe_w9WbV3J",
        "colab_type": "code",
        "outputId": "e1860bff-0f34-4fdd-a866-b9f7ac91991c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()# Loss function\n",
        "optimizer = torch.optim.Adam(vgg16.classifier.parameters(), lr=1e-3)# Optimizer\n",
        "\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):        \n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vgg16(images)\n",
        "        #print(outputs.shape)\n",
        "        #break\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 1 == 0:\n",
        "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
        "                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [2/5], Step: [19/500], Loss: 1.4260\n",
            "Epoch: [2/5], Step: [20/500], Loss: 1.8924\n",
            "Epoch: [2/5], Step: [21/500], Loss: 1.5116\n",
            "Epoch: [2/5], Step: [22/500], Loss: 1.8302\n",
            "Epoch: [2/5], Step: [23/500], Loss: 1.7025\n",
            "Epoch: [2/5], Step: [24/500], Loss: 2.9106\n",
            "Epoch: [2/5], Step: [25/500], Loss: 1.9810\n",
            "Epoch: [2/5], Step: [26/500], Loss: 1.4249\n",
            "Epoch: [2/5], Step: [27/500], Loss: 2.1832\n",
            "Epoch: [2/5], Step: [28/500], Loss: 1.7107\n",
            "Epoch: [2/5], Step: [29/500], Loss: 1.9202\n",
            "Epoch: [2/5], Step: [30/500], Loss: 1.5315\n",
            "Epoch: [2/5], Step: [31/500], Loss: 1.7533\n",
            "Epoch: [2/5], Step: [32/500], Loss: 2.1113\n",
            "Epoch: [2/5], Step: [33/500], Loss: 1.6176\n",
            "Epoch: [2/5], Step: [34/500], Loss: 1.7087\n",
            "Epoch: [2/5], Step: [35/500], Loss: 2.4554\n",
            "Epoch: [2/5], Step: [36/500], Loss: 2.0677\n",
            "Epoch: [2/5], Step: [37/500], Loss: 2.0490\n",
            "Epoch: [2/5], Step: [38/500], Loss: 1.7464\n",
            "Epoch: [2/5], Step: [39/500], Loss: 1.7873\n",
            "Epoch: [2/5], Step: [40/500], Loss: 2.1472\n",
            "Epoch: [2/5], Step: [41/500], Loss: 1.7758\n",
            "Epoch: [2/5], Step: [42/500], Loss: 1.7034\n",
            "Epoch: [2/5], Step: [43/500], Loss: 1.9933\n",
            "Epoch: [2/5], Step: [44/500], Loss: 2.4338\n",
            "Epoch: [2/5], Step: [45/500], Loss: 2.0252\n",
            "Epoch: [2/5], Step: [46/500], Loss: 2.2673\n",
            "Epoch: [2/5], Step: [47/500], Loss: 1.6841\n",
            "Epoch: [2/5], Step: [48/500], Loss: 2.3312\n",
            "Epoch: [2/5], Step: [49/500], Loss: 2.7997\n",
            "Epoch: [2/5], Step: [50/500], Loss: 1.8529\n",
            "Epoch: [2/5], Step: [51/500], Loss: 2.2901\n",
            "Epoch: [2/5], Step: [52/500], Loss: 2.5200\n",
            "Epoch: [2/5], Step: [53/500], Loss: 2.1813\n",
            "Epoch: [2/5], Step: [54/500], Loss: 1.8231\n",
            "Epoch: [2/5], Step: [55/500], Loss: 1.6685\n",
            "Epoch: [2/5], Step: [56/500], Loss: 1.7935\n",
            "Epoch: [2/5], Step: [57/500], Loss: 1.5574\n",
            "Epoch: [2/5], Step: [58/500], Loss: 1.7989\n",
            "Epoch: [2/5], Step: [59/500], Loss: 1.9293\n",
            "Epoch: [2/5], Step: [60/500], Loss: 1.8054\n",
            "Epoch: [2/5], Step: [61/500], Loss: 1.4745\n",
            "Epoch: [2/5], Step: [62/500], Loss: 1.6613\n",
            "Epoch: [2/5], Step: [63/500], Loss: 2.4374\n",
            "Epoch: [2/5], Step: [64/500], Loss: 2.2491\n",
            "Epoch: [2/5], Step: [65/500], Loss: 2.1896\n",
            "Epoch: [2/5], Step: [66/500], Loss: 2.0076\n",
            "Epoch: [2/5], Step: [67/500], Loss: 1.7658\n",
            "Epoch: [2/5], Step: [68/500], Loss: 2.4705\n",
            "Epoch: [2/5], Step: [69/500], Loss: 1.9372\n",
            "Epoch: [2/5], Step: [70/500], Loss: 2.4593\n",
            "Epoch: [2/5], Step: [71/500], Loss: 1.8158\n",
            "Epoch: [2/5], Step: [72/500], Loss: 1.5141\n",
            "Epoch: [2/5], Step: [73/500], Loss: 2.0428\n",
            "Epoch: [2/5], Step: [74/500], Loss: 2.9852\n",
            "Epoch: [2/5], Step: [75/500], Loss: 2.0852\n",
            "Epoch: [2/5], Step: [76/500], Loss: 2.0851\n",
            "Epoch: [2/5], Step: [77/500], Loss: 2.2154\n",
            "Epoch: [2/5], Step: [78/500], Loss: 2.2095\n",
            "Epoch: [2/5], Step: [79/500], Loss: 2.6475\n",
            "Epoch: [2/5], Step: [80/500], Loss: 2.1686\n",
            "Epoch: [2/5], Step: [81/500], Loss: 2.0839\n",
            "Epoch: [2/5], Step: [82/500], Loss: 1.5968\n",
            "Epoch: [2/5], Step: [83/500], Loss: 2.2202\n",
            "Epoch: [2/5], Step: [84/500], Loss: 1.9868\n",
            "Epoch: [2/5], Step: [85/500], Loss: 1.7846\n",
            "Epoch: [2/5], Step: [86/500], Loss: 2.4835\n",
            "Epoch: [2/5], Step: [87/500], Loss: 1.9381\n",
            "Epoch: [2/5], Step: [88/500], Loss: 2.5225\n",
            "Epoch: [2/5], Step: [89/500], Loss: 2.1558\n",
            "Epoch: [2/5], Step: [90/500], Loss: 1.9693\n",
            "Epoch: [2/5], Step: [91/500], Loss: 1.3701\n",
            "Epoch: [2/5], Step: [92/500], Loss: 2.0448\n",
            "Epoch: [2/5], Step: [93/500], Loss: 2.3063\n",
            "Epoch: [2/5], Step: [94/500], Loss: 1.4443\n",
            "Epoch: [2/5], Step: [95/500], Loss: 2.0937\n",
            "Epoch: [2/5], Step: [96/500], Loss: 2.1436\n",
            "Epoch: [2/5], Step: [97/500], Loss: 2.0434\n",
            "Epoch: [2/5], Step: [98/500], Loss: 2.4701\n",
            "Epoch: [2/5], Step: [99/500], Loss: 2.0986\n",
            "Epoch: [2/5], Step: [100/500], Loss: 2.6476\n",
            "Epoch: [2/5], Step: [101/500], Loss: 1.4217\n",
            "Epoch: [2/5], Step: [102/500], Loss: 1.9990\n",
            "Epoch: [2/5], Step: [103/500], Loss: 2.2719\n",
            "Epoch: [2/5], Step: [104/500], Loss: 2.3889\n",
            "Epoch: [2/5], Step: [105/500], Loss: 1.6482\n",
            "Epoch: [2/5], Step: [106/500], Loss: 1.7899\n",
            "Epoch: [2/5], Step: [107/500], Loss: 1.7910\n",
            "Epoch: [2/5], Step: [108/500], Loss: 2.1757\n",
            "Epoch: [2/5], Step: [109/500], Loss: 1.8135\n",
            "Epoch: [2/5], Step: [110/500], Loss: 2.1368\n",
            "Epoch: [2/5], Step: [111/500], Loss: 2.1365\n",
            "Epoch: [2/5], Step: [112/500], Loss: 1.9497\n",
            "Epoch: [2/5], Step: [113/500], Loss: 1.5408\n",
            "Epoch: [2/5], Step: [114/500], Loss: 1.5452\n",
            "Epoch: [2/5], Step: [115/500], Loss: 2.0657\n",
            "Epoch: [2/5], Step: [116/500], Loss: 1.7065\n",
            "Epoch: [2/5], Step: [117/500], Loss: 2.2757\n",
            "Epoch: [2/5], Step: [118/500], Loss: 1.8905\n",
            "Epoch: [2/5], Step: [119/500], Loss: 1.7424\n",
            "Epoch: [2/5], Step: [120/500], Loss: 1.5702\n",
            "Epoch: [2/5], Step: [121/500], Loss: 1.6779\n",
            "Epoch: [2/5], Step: [122/500], Loss: 2.0914\n",
            "Epoch: [2/5], Step: [123/500], Loss: 2.0714\n",
            "Epoch: [2/5], Step: [124/500], Loss: 1.7058\n",
            "Epoch: [2/5], Step: [125/500], Loss: 1.9824\n",
            "Epoch: [2/5], Step: [126/500], Loss: 1.7724\n",
            "Epoch: [2/5], Step: [127/500], Loss: 2.1232\n",
            "Epoch: [2/5], Step: [128/500], Loss: 1.7562\n",
            "Epoch: [2/5], Step: [129/500], Loss: 2.3979\n",
            "Epoch: [2/5], Step: [130/500], Loss: 1.7088\n",
            "Epoch: [2/5], Step: [131/500], Loss: 1.7856\n",
            "Epoch: [2/5], Step: [132/500], Loss: 1.9014\n",
            "Epoch: [2/5], Step: [133/500], Loss: 2.3044\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}