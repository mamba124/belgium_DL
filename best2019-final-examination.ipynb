{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of best2019-final-examination.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/glouppe/info8010-deep-learning/blob/master/tutorials/best2019-final-examination.ipynb","timestamp":1563265670542}],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AWU8Taryp_Rb","colab_type":"text"},"source":["# BEST 2019 Final examination\n","\n","![](https://github.com/glouppe/info8010-deep-learning/blob/master/tutorials/images/style_transfer.png?raw=1)\n","\n","## Neural Style Transfer\n","\n","The goal of your project is to reimplement the algorithm proposed in: [A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)\n","\n","We expect you to come up with a working implementation of the original algorithm that is based on the following steps.\n","\n","This will correspond to your **minimum viable product!**.\n","\n","- Given any natural and stylized images, the goal is to produce a new image which keeps the content of the first one and applies the style of the latter.\n","\n","- We expect you to work with a pretrained neural network (e.g. VGG16, ResNet...) which will serve as a feature extractor: to do so you will have to come up with your own forward() function.\n","\n","- You will have to reimplement the different losses presented in the paper.\n","\n","- Produce at least one good-looking sample for the content image you were given.\n","\n","**BONUS:**\n","\n","- Play with the different layers used for extracting the features.\n","\n","- Investigate how to balance the preservation of the content with respect to the transferred style.\n","\n","- Come up with a way that extends the algorithm to the use of multiple styles.\n","\n","- Investigate the differences between starting with a pretrained network and one which is randomly initialized.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GwSXrApIp_Re","colab_type":"text"},"source":["## Non-exhaustive summary of [A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf)"]},{"cell_type":"markdown","metadata":{"id":"iABcNGe9p_Rf","colab_type":"text"},"source":["The authors propose to cast the style transfer problem as an optimization procedure over the pixels of the target image. To do so they define a double loss composed of a content loss and style loss:\n","\n","- The content loss is mathematically defined as the mean squared error (MSE) between the two feature maps (target and content images) over the layers. \n","- The style loss can be computed with the MSE between the gram matrices of the vectorized feature maps (one vector per channel). \n","\n","The two losses are then combined and minimized with gradient descent."]},{"cell_type":"code","metadata":{"id":"4D2Vsc8Dp_Rg","colab_type":"code","colab":{}},"source":["from torchvision import models\n","from torchvision import transforms\n","from PIL import Image\n","import argparse\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","from types import SimpleNamespace"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WnTqVpeGp_Ro","colab_type":"text"},"source":["### Experimental Setting"]},{"cell_type":"code","metadata":{"id":"ACnM-yXNp_Rp","colab_type":"code","colab":{}},"source":["config = SimpleNamespace()\n","config.content = '1.jpg'\n","config.style = 'style.jpg'\n","config.style2 = 'box.jpg'\n","config.max_size = 400\n","config.total_step = 2000\n","config.log_step = 10\n","config.sample_step = 200\n","config.style_weight = 100\n","config.lr = .003"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"24y6AHjzp_Rt","colab_type":"code","colab":{}},"source":["# Device configuration\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtAcs7CEp_Rw","colab_type":"text"},"source":["### Architecture"]},{"cell_type":"markdown","metadata":{"id":"XVQTeBCpp_Rx","colab_type":"text"},"source":["Define your favourite neural network which will serve as [features extractor](https://pytorch.org/docs/stable/torchvision/models.html).\n","\n","Redefine your forward pass for computing the features from the network.\n"]},{"cell_type":"code","metadata":{"id":"kW3nMsEWp_Ry","colab_type":"code","colab":{}},"source":["class PretrainedNet(nn.Module):\n","    def __init__(self):\n","        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n","        super(PretrainedNet, self).__init__()\n","        self.select = [0, 5, 7, 23, 9]\n","        self.pretrainedNet = models.vgg19(pretrained=True).to(device)\n","        \n","    def forward(self, x):\n","        \"\"\"Extract multiple (5 is good) convolutional feature maps.\"\"\"\n","        features = []\n","        output = x\n","        for layer_ind in range(len(self.pretrainedNet.features)):\n","          output = self.pretrainedNet.features[layer_ind](output)\n","          if layer_ind in self.select:\n","            features.append(output)\n","        return features"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ETnJnlYKp_R2","colab_type":"text"},"source":["### Image loader"]},{"cell_type":"code","metadata":{"id":"lxaqKeDDp_R4","colab_type":"code","colab":{}},"source":["def load_image(image_path, transform=None, max_size=None, shape=None):\n","    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n","    image = Image.open(image_path)\n","    \n","    if max_size:\n","        scale = max_size / max(image.size)\n","        size = np.array(image.size) * scale\n","        image = image.resize(size.astype(int), Image.ANTIALIAS)\n","    \n","    if shape:\n","        image = image.resize(shape, Image.LANCZOS)\n","    \n","    if transform:\n","        image = transform(image).unsqueeze(0)\n","    \n","    return image.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WU26KxTDp_R7","colab_type":"text"},"source":["The pytorchvision pretrained models are trained on ImageNet where images are normalized by `mean=[0.485, 0.456, 0.406]` and `std=[0.229, 0.224, 0.225]`. We use the same normalization statistics here."]},{"cell_type":"code","metadata":{"id":"hHwoXKwlp_R9","colab_type":"code","colab":{}},"source":["transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.485, 0.456, 0.406), \n","                                                                            std=(0.229, 0.224, 0.225))])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqpBeOxhp_SB","colab_type":"text"},"source":["Load content and style images.\n","Make the style image same size as the content image.\n"]},{"cell_type":"code","metadata":{"id":"TcijlkXKp_SE","colab_type":"code","colab":{}},"source":["content = load_image(config.content, transform, max_size=config.max_size)\n","style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n","style2 = load_image(config.style2, transform, shape=[content.size(2), content.size(3)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYDbqv_Zp_SM","colab_type":"text"},"source":["### Optimization"]},{"cell_type":"markdown","metadata":{"id":"VwB8a69cp_SO","colab_type":"text"},"source":["Initialize a target image with the content image"]},{"cell_type":"code","metadata":{"id":"XUNeufG0p_SP","colab_type":"code","colab":{}},"source":["target = content.clone().requires_grad_(True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-1HH4Nup_SW","colab_type":"code","colab":{}},"source":["net = PretrainedNet().eval()\n","optimizer = torch.optim.Adam([target], lr=0.1)\n","content_criteria = nn.MSELoss()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jA-BIXPip_SZ","colab_type":"text"},"source":["The main training loop of the algorithm: we separately deal with the content and style losses (be careful to vectorize your feature maps and follow the formulas presented in the paper)."]},{"cell_type":"code","metadata":{"id":"u0tScbwyp_Sa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4cee1694-d1ae-4c50-e767-9954888a0ddb","executionInfo":{"status":"ok","timestamp":1563290496614,"user_tz":-120,"elapsed":1155806,"user":{"displayName":"Tomis Polisciuc","photoUrl":"","userId":"16586180875053408915"}}},"source":["for step in range(config.total_step):\n","    # Extract multiple(5) feature maps\n","    target_features = net.forward(target)\n","    content_features = net.forward(content) \n","    style_features1 = net.forward(style)\n","    style_features2 = net.forward(style2)\n","    \n","\n","    style_loss = 0\n","    content_loss = 0\n","\n","    for f1, f2, f3,f4 in zip(target_features, content_features, style_features1,style_features2 ):\n","        # Compute content loss with target and content images\n","        content_loss += content_criteria(f1, f2)\n","        #print(content_loss)\n","\n","        # Reshape convolutional feature maps\n","        _, c, h, w = f1.size()\n","        f1 = f1.reshape(c, h*w).to(device)\n","        f3 = f3.reshape(c, h*w).to(device)\n","        f4 = f3.reshape(c, h*w).to(device)\n","\n","        # Compute gram matrix\n","        f1 = torch.mm(f1, f1.t())\n","        f3 = torch.mm(f3, f3.t())\n","        f4 = torch.mm(f4, f4.t())\n","\n","        # Compute style loss with target and style images\n","        style_loss += (1/(len(target_features)*len(content_features))**2)*content_criteria(f1,f3) + (1/(len(target_features)*len(content_features))**2)*content_criteria(f1,f4)\n","    \n","    # Compute total loss, backprop and optimize (4 lines of code in total)\n","    loss = style_loss + len(target_features)*len(content_features)**2*content_loss\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Output\n","    if (step+1) % config.log_step == 0:\n","        # Change this print into a log with tensorboardx\n","        print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n","               .format(step+1, config.total_step, content_loss.item(), style_loss.item()))\n","\n","    if (step+1) % 50 == 0:\n","        # Save the generated image (you can also change it to see it with tensorboardx)\n","        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n","        img = target.clone().squeeze()\n","        img = denorm(img).clamp_(0, 1)\n","        torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))"],"execution_count":156,"outputs":[{"output_type":"stream","text":["Step [10/2000], Content Loss: 49.8202, Style Loss: 21865538.0000\n","Step [20/2000], Content Loss: 54.5206, Style Loss: 11413899.0000\n","Step [30/2000], Content Loss: 56.5388, Style Loss: 6625276.5000\n","Step [40/2000], Content Loss: 57.5198, Style Loss: 4324963.5000\n","Step [50/2000], Content Loss: 58.2117, Style Loss: 3129509.0000\n","Step [60/2000], Content Loss: 58.9963, Style Loss: 2431456.2500\n","Step [70/2000], Content Loss: 59.6289, Style Loss: 1982983.0000\n","Step [80/2000], Content Loss: 60.1768, Style Loss: 1671017.8750\n","Step [90/2000], Content Loss: 60.5980, Style Loss: 1438556.6250\n","Step [100/2000], Content Loss: 60.9913, Style Loss: 1257589.8750\n","Step [110/2000], Content Loss: 61.3521, Style Loss: 1110649.8750\n","Step [120/2000], Content Loss: 61.6712, Style Loss: 988609.3750\n","Step [130/2000], Content Loss: 61.9644, Style Loss: 885282.0625\n","Step [140/2000], Content Loss: 62.2082, Style Loss: 797097.0000\n","Step [150/2000], Content Loss: 62.4270, Style Loss: 721104.4375\n","Step [160/2000], Content Loss: 62.6325, Style Loss: 655312.2500\n","Step [170/2000], Content Loss: 62.8246, Style Loss: 598304.3750\n","Step [180/2000], Content Loss: 63.0044, Style Loss: 548834.3125\n","Step [190/2000], Content Loss: 63.1789, Style Loss: 505780.5312\n","Step [200/2000], Content Loss: 63.3489, Style Loss: 468238.4062\n","Step [210/2000], Content Loss: 63.5168, Style Loss: 435394.3438\n","Step [220/2000], Content Loss: 63.6756, Style Loss: 406477.7500\n","Step [230/2000], Content Loss: 63.8213, Style Loss: 380956.3438\n","Step [240/2000], Content Loss: 63.9648, Style Loss: 358402.1875\n","Step [250/2000], Content Loss: 64.1016, Style Loss: 338216.5938\n","Step [260/2000], Content Loss: 64.2377, Style Loss: 320029.0312\n","Step [270/2000], Content Loss: 64.3612, Style Loss: 303631.2188\n","Step [280/2000], Content Loss: 64.4788, Style Loss: 288774.2500\n","Step [290/2000], Content Loss: 64.5922, Style Loss: 275244.9375\n","Step [300/2000], Content Loss: 64.7065, Style Loss: 262883.3438\n","Step [310/2000], Content Loss: 64.8098, Style Loss: 251500.2812\n","Step [320/2000], Content Loss: 64.9098, Style Loss: 240958.5312\n","Step [330/2000], Content Loss: 65.0103, Style Loss: 231213.3438\n","Step [340/2000], Content Loss: 65.1126, Style Loss: 222138.5469\n","Step [350/2000], Content Loss: 65.2123, Style Loss: 213663.8281\n","Step [360/2000], Content Loss: 65.3132, Style Loss: 205778.5312\n","Step [370/2000], Content Loss: 65.4104, Style Loss: 198394.7500\n","Step [380/2000], Content Loss: 65.4951, Style Loss: 191489.3594\n","Step [390/2000], Content Loss: 65.5767, Style Loss: 184992.2969\n","Step [400/2000], Content Loss: 65.6530, Style Loss: 178880.5938\n","Step [410/2000], Content Loss: 65.7337, Style Loss: 173082.5312\n","Step [420/2000], Content Loss: 65.8190, Style Loss: 167589.1875\n","Step [430/2000], Content Loss: 65.8963, Style Loss: 162387.6562\n","Step [440/2000], Content Loss: 65.9679, Style Loss: 157426.3906\n","Step [450/2000], Content Loss: 66.0386, Style Loss: 152699.5625\n","Step [460/2000], Content Loss: 66.1133, Style Loss: 148190.8750\n","Step [470/2000], Content Loss: 66.1877, Style Loss: 143872.5781\n","Step [480/2000], Content Loss: 66.2576, Style Loss: 139734.9062\n","Step [490/2000], Content Loss: 66.3285, Style Loss: 135784.2969\n","Step [500/2000], Content Loss: 66.3939, Style Loss: 132032.3906\n","Step [510/2000], Content Loss: 66.4570, Style Loss: 128457.2266\n","Step [520/2000], Content Loss: 66.5275, Style Loss: 125045.6719\n","Step [530/2000], Content Loss: 66.5960, Style Loss: 121774.8594\n","Step [540/2000], Content Loss: 66.6638, Style Loss: 118640.6641\n","Step [550/2000], Content Loss: 66.7246, Style Loss: 115649.4141\n","Step [560/2000], Content Loss: 66.7870, Style Loss: 112782.1406\n","Step [570/2000], Content Loss: 66.8463, Style Loss: 110036.8594\n","Step [580/2000], Content Loss: 66.9030, Style Loss: 107413.8125\n","Step [590/2000], Content Loss: 66.9625, Style Loss: 104898.5156\n","Step [600/2000], Content Loss: 67.0206, Style Loss: 102489.2812\n","Step [610/2000], Content Loss: 67.0732, Style Loss: 100185.6094\n","Step [620/2000], Content Loss: 67.1221, Style Loss: 97977.8281\n","Step [630/2000], Content Loss: 67.1730, Style Loss: 95840.3125\n","Step [640/2000], Content Loss: 67.2271, Style Loss: 93762.0391\n","Step [650/2000], Content Loss: 67.2791, Style Loss: 91771.7578\n","Step [660/2000], Content Loss: 67.3312, Style Loss: 89841.0703\n","Step [670/2000], Content Loss: 67.3832, Style Loss: 87991.8359\n","Step [680/2000], Content Loss: 67.4308, Style Loss: 86209.9609\n","Step [690/2000], Content Loss: 67.4787, Style Loss: 84491.8359\n","Step [700/2000], Content Loss: 67.5237, Style Loss: 82841.2266\n","Step [710/2000], Content Loss: 67.5710, Style Loss: 81249.1406\n","Step [720/2000], Content Loss: 67.6179, Style Loss: 79707.0859\n","Step [730/2000], Content Loss: 67.6621, Style Loss: 78220.8828\n","Step [740/2000], Content Loss: 67.7066, Style Loss: 76787.1562\n","Step [750/2000], Content Loss: 67.7489, Style Loss: 75390.5859\n","Step [760/2000], Content Loss: 67.7912, Style Loss: 74043.6094\n","Step [770/2000], Content Loss: 67.8320, Style Loss: 72734.3125\n","Step [780/2000], Content Loss: 67.8704, Style Loss: 71465.3047\n","Step [790/2000], Content Loss: 67.9170, Style Loss: 70221.0781\n","Step [800/2000], Content Loss: 67.9590, Style Loss: 69024.1797\n","Step [810/2000], Content Loss: 67.9997, Style Loss: 67856.2578\n","Step [820/2000], Content Loss: 68.0379, Style Loss: 66727.7031\n","Step [830/2000], Content Loss: 68.0761, Style Loss: 65621.9688\n","Step [840/2000], Content Loss: 68.1143, Style Loss: 64544.4297\n","Step [850/2000], Content Loss: 68.1510, Style Loss: 63503.7852\n","Step [860/2000], Content Loss: 68.1851, Style Loss: 62495.9648\n","Step [870/2000], Content Loss: 68.2193, Style Loss: 61508.7148\n","Step [880/2000], Content Loss: 68.2559, Style Loss: 60550.8164\n","Step [890/2000], Content Loss: 68.2872, Style Loss: 59613.3125\n","Step [900/2000], Content Loss: 68.3210, Style Loss: 58702.9844\n","Step [910/2000], Content Loss: 68.3569, Style Loss: 57820.4102\n","Step [920/2000], Content Loss: 68.3899, Style Loss: 56966.6914\n","Step [930/2000], Content Loss: 68.4324, Style Loss: 56164.3555\n","Step [940/2000], Content Loss: 68.5201, Style Loss: 56360.0547\n","Step [950/2000], Content Loss: 66.6359, Style Loss: 916895.0000\n","Step [960/2000], Content Loss: 65.6810, Style Loss: 1209040.5000\n","Step [970/2000], Content Loss: 66.2575, Style Loss: 534051.6250\n","Step [980/2000], Content Loss: 67.2338, Style Loss: 296924.7188\n","Step [990/2000], Content Loss: 67.3297, Style Loss: 244776.0781\n","Step [1000/2000], Content Loss: 67.6935, Style Loss: 193664.4375\n","Step [1010/2000], Content Loss: 68.1093, Style Loss: 164611.3594\n","Step [1020/2000], Content Loss: 68.2846, Style Loss: 152757.9219\n","Step [1030/2000], Content Loss: 67.7139, Style Loss: 238624.6094\n","Step [1040/2000], Content Loss: 69.7462, Style Loss: 195905.0938\n","Step [1050/2000], Content Loss: 68.0519, Style Loss: 195973.8281\n","Step [1060/2000], Content Loss: 69.4361, Style Loss: 155491.5312\n","Step [1070/2000], Content Loss: 68.3800, Style Loss: 158413.7344\n","Step [1080/2000], Content Loss: 69.5930, Style Loss: 127855.6406\n","Step [1090/2000], Content Loss: 68.8095, Style Loss: 108106.8984\n","Step [1100/2000], Content Loss: 69.6171, Style Loss: 92451.1016\n","Step [1110/2000], Content Loss: 69.0827, Style Loss: 83673.4219\n","Step [1120/2000], Content Loss: 69.5798, Style Loss: 69889.9609\n","Step [1130/2000], Content Loss: 69.4833, Style Loss: 63470.8086\n","Step [1140/2000], Content Loss: 69.5332, Style Loss: 59692.5391\n","Step [1150/2000], Content Loss: 69.6258, Style Loss: 56593.5273\n","Step [1160/2000], Content Loss: 69.6802, Style Loss: 54338.9922\n","Step [1170/2000], Content Loss: 69.7242, Style Loss: 52496.7422\n","Step [1180/2000], Content Loss: 69.7604, Style Loss: 50927.9883\n","Step [1190/2000], Content Loss: 69.7975, Style Loss: 49549.2305\n","Step [1200/2000], Content Loss: 69.8334, Style Loss: 48312.8594\n","Step [1210/2000], Content Loss: 69.8702, Style Loss: 47183.1289\n","Step [1220/2000], Content Loss: 69.8984, Style Loss: 46142.2500\n","Step [1230/2000], Content Loss: 69.9263, Style Loss: 45179.0391\n","Step [1240/2000], Content Loss: 69.9550, Style Loss: 44280.6094\n","Step [1250/2000], Content Loss: 69.9827, Style Loss: 43435.4375\n","Step [1260/2000], Content Loss: 70.0114, Style Loss: 42637.7031\n","Step [1270/2000], Content Loss: 70.0403, Style Loss: 41886.7109\n","Step [1280/2000], Content Loss: 70.0655, Style Loss: 41172.9805\n","Step [1290/2000], Content Loss: 70.0911, Style Loss: 40491.1719\n","Step [1300/2000], Content Loss: 70.1190, Style Loss: 39841.3086\n","Step [1310/2000], Content Loss: 70.1438, Style Loss: 39222.5430\n","Step [1320/2000], Content Loss: 70.1704, Style Loss: 38630.2500\n","Step [1330/2000], Content Loss: 70.1958, Style Loss: 38061.1953\n","Step [1340/2000], Content Loss: 70.2207, Style Loss: 37512.2422\n","Step [1350/2000], Content Loss: 70.2491, Style Loss: 36990.1016\n","Step [1360/2000], Content Loss: 70.2779, Style Loss: 36493.8789\n","Step [1370/2000], Content Loss: 70.3000, Style Loss: 36019.2266\n","Step [1380/2000], Content Loss: 70.3253, Style Loss: 35572.8867\n","Step [1390/2000], Content Loss: 70.3474, Style Loss: 35137.4727\n","Step [1400/2000], Content Loss: 70.3724, Style Loss: 34744.1914\n","Step [1410/2000], Content Loss: 70.3965, Style Loss: 34342.1914\n","Step [1420/2000], Content Loss: 70.4456, Style Loss: 34198.6992\n","Step [1430/2000], Content Loss: 70.6457, Style Loss: 84093.5391\n","Step [1440/2000], Content Loss: 67.5309, Style Loss: 1691058.8750\n","Step [1450/2000], Content Loss: 67.6395, Style Loss: 677733.8750\n","Step [1460/2000], Content Loss: 68.9401, Style Loss: 335430.7500\n","Step [1470/2000], Content Loss: 69.2516, Style Loss: 208552.8594\n","Step [1480/2000], Content Loss: 69.8267, Style Loss: 146404.7500\n","Step [1490/2000], Content Loss: 70.6804, Style Loss: 130220.5703\n","Step [1500/2000], Content Loss: 69.8357, Style Loss: 111882.0625\n","Step [1510/2000], Content Loss: 70.4028, Style Loss: 92651.6641\n","Step [1520/2000], Content Loss: 70.6516, Style Loss: 102896.3047\n","Step [1530/2000], Content Loss: 71.4559, Style Loss: 167955.8125\n","Step [1540/2000], Content Loss: 70.9017, Style Loss: 127465.1250\n","Step [1550/2000], Content Loss: 70.1313, Style Loss: 135360.7344\n","Step [1560/2000], Content Loss: 70.6480, Style Loss: 131513.2031\n","Step [1570/2000], Content Loss: 69.9291, Style Loss: 137209.6250\n","Step [1580/2000], Content Loss: 70.9718, Style Loss: 101244.4062\n","Step [1590/2000], Content Loss: 70.2889, Style Loss: 88544.0938\n","Step [1600/2000], Content Loss: 71.0082, Style Loss: 71676.0312\n","Step [1610/2000], Content Loss: 70.5884, Style Loss: 62467.5195\n","Step [1620/2000], Content Loss: 71.0125, Style Loss: 54439.5078\n","Step [1630/2000], Content Loss: 70.8506, Style Loss: 49739.1641\n","Step [1640/2000], Content Loss: 70.9815, Style Loss: 46270.5273\n","Step [1650/2000], Content Loss: 71.0376, Style Loss: 44058.8750\n","Step [1660/2000], Content Loss: 71.0650, Style Loss: 42303.9883\n","Step [1670/2000], Content Loss: 71.0937, Style Loss: 40841.5547\n","Step [1680/2000], Content Loss: 71.1166, Style Loss: 39584.7188\n","Step [1690/2000], Content Loss: 71.1374, Style Loss: 38477.0352\n","Step [1700/2000], Content Loss: 71.1640, Style Loss: 37488.9570\n","Step [1710/2000], Content Loss: 71.1895, Style Loss: 36594.3633\n","Step [1720/2000], Content Loss: 71.2123, Style Loss: 35776.1953\n","Step [1730/2000], Content Loss: 71.2361, Style Loss: 35016.2148\n","Step [1740/2000], Content Loss: 71.2566, Style Loss: 34310.4844\n","Step [1750/2000], Content Loss: 71.2768, Style Loss: 33651.3867\n","Step [1760/2000], Content Loss: 71.2962, Style Loss: 33034.5703\n","Step [1770/2000], Content Loss: 71.3149, Style Loss: 32454.9219\n","Step [1780/2000], Content Loss: 71.3350, Style Loss: 31907.5078\n","Step [1790/2000], Content Loss: 71.3528, Style Loss: 31388.0391\n","Step [1800/2000], Content Loss: 71.3721, Style Loss: 30898.3633\n","Step [1810/2000], Content Loss: 71.3945, Style Loss: 30447.3398\n","Step [1820/2000], Content Loss: 71.4144, Style Loss: 30026.1348\n","Step [1830/2000], Content Loss: 71.4299, Style Loss: 29609.1426\n","Step [1840/2000], Content Loss: 71.4673, Style Loss: 29357.8008\n","Step [1850/2000], Content Loss: 71.5352, Style Loss: 29728.7637\n","Step [1860/2000], Content Loss: 71.6562, Style Loss: 414380.2500\n","Step [1870/2000], Content Loss: 69.0845, Style Loss: 991902.6875\n","Step [1880/2000], Content Loss: 70.1289, Style Loss: 470130.4062\n","Step [1890/2000], Content Loss: 70.1946, Style Loss: 248618.1250\n","Step [1900/2000], Content Loss: 69.7202, Style Loss: 204776.8594\n","Step [1910/2000], Content Loss: 71.3428, Style Loss: 139971.7969\n","Step [1920/2000], Content Loss: 70.5569, Style Loss: 114678.1875\n","Step [1930/2000], Content Loss: 71.4090, Style Loss: 88126.6172\n","Step [1940/2000], Content Loss: 70.9660, Style Loss: 76005.7656\n","Step [1950/2000], Content Loss: 71.4622, Style Loss: 64980.9492\n","Step [1960/2000], Content Loss: 71.2514, Style Loss: 58504.0156\n","Step [1970/2000], Content Loss: 71.4985, Style Loss: 53089.6758\n","Step [1980/2000], Content Loss: 71.4889, Style Loss: 49395.1484\n","Step [1990/2000], Content Loss: 71.5341, Style Loss: 46653.7305\n","Step [2000/2000], Content Loss: 71.5776, Style Loss: 44421.2422\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i8qRPx5Ep_Sf","colab_type":"text"},"source":["## Bonus"]},{"cell_type":"markdown","metadata":{"id":"h7W3eT8jp_Sg","colab_type":"text"},"source":["<div class=\"alert alert-danger\">\n","<b>EXERCISE</b>:\n","\n","\n","Play with the different layers used for extracting the features.\n","\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"Z8fJAmzPp_Sh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1HlOX6tp_Sk","colab_type":"text"},"source":["<div class=\"alert alert-danger\">\n","<b>EXERCISE</b>:\n","    \n","Investigate how to balance the preservation of the content with respect to the transferred style.\n","\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"Ac1Q_TzEp_Sk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qy2hjU1p_Sq","colab_type":"text"},"source":["<div class=\"alert alert-danger\">\n","<b>EXERCISE</b>:\n","\n","Come up with a way that extends the algorithm to the use of multiple styles.\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"LzTbd39Hp_Sr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-xBv91K4p_Su","colab_type":"text"},"source":["<div class=\"alert alert-danger\">\n","<b>EXERCISE</b>:\n","\n"," Investigate the differences between starting with a pretrained network and one which is randomly initialized.\n","\n","\n","</div>"]},{"cell_type":"code","metadata":{"id":"rkDSKKXTp_Sv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}