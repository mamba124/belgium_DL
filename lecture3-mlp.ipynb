{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used as part of INFO8010 Deep Learning (Gilles Louppe, 2018-2019).\n",
    "- Originally adapted from [Pytorch tutorial for Deep Learning researchers](https://github.com/yunjey/pytorch-tutorial) (Yunvey Choi, 2018).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()#inheritance\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 1.2657\n",
      "Epoch: [1/5], Step: [200/600], Loss: 1.3377\n",
      "Epoch: [1/5], Step: [300/600], Loss: 1.3427\n",
      "Epoch: [1/5], Step: [400/600], Loss: 1.2121\n",
      "Epoch: [1/5], Step: [500/600], Loss: 1.2958\n",
      "Epoch: [1/5], Step: [600/600], Loss: 1.1308\n",
      "Epoch: [2/5], Step: [100/600], Loss: 1.1277\n",
      "Epoch: [2/5], Step: [200/600], Loss: 1.0630\n",
      "Epoch: [2/5], Step: [300/600], Loss: 1.1928\n",
      "Epoch: [2/5], Step: [400/600], Loss: 1.0932\n",
      "Epoch: [2/5], Step: [500/600], Loss: 1.0605\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.0625\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.1545\n",
      "Epoch: [3/5], Step: [200/600], Loss: 0.8863\n",
      "Epoch: [3/5], Step: [300/600], Loss: 0.9181\n",
      "Epoch: [3/5], Step: [400/600], Loss: 0.9937\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.0642\n",
      "Epoch: [3/5], Step: [600/600], Loss: 0.9100\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.0181\n",
      "Epoch: [4/5], Step: [200/600], Loss: 0.8795\n",
      "Epoch: [4/5], Step: [300/600], Loss: 0.9071\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.0159\n",
      "Epoch: [4/5], Step: [500/600], Loss: 0.9321\n",
      "Epoch: [4/5], Step: [600/600], Loss: 0.8920\n",
      "Epoch: [5/5], Step: [100/600], Loss: 0.8407\n",
      "Epoch: [5/5], Step: [200/600], Loss: 0.8338\n",
      "Epoch: [5/5], Step: [300/600], Loss: 0.8339\n",
      "Epoch: [5/5], Step: [400/600], Loss: 0.7984\n",
      "Epoch: [5/5], Step: [500/600], Loss: 0.7744\n",
      "Epoch: [5/5], Step: [600/600], Loss: 0.7955\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_tot = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_tot+=loss.detach().data\n",
    "        writer.add_scalar('data/loss',loss.detach().data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, evaluating the learning of your model ask you to take a look at the loss evolution during training time. [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard), more specifically [tensorboardX](https://github.com/lanpa/tensorboardX) on pytorch, is a toolkit which is made to simplify this procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in c:\\programdata\\anaconda3\\lib\\site-packages (1.8)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboardX) (1.16.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboardX) (3.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data logger\n",
    "writer = SummaryWriter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    writer.add_scalar('data/scalar1', i**2, i)\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "\n",
    "Now that you have seen how this handy tool works, you can monitor the progress of your neural networks within it. \n",
    "Redirect what you think are the most informative logs of your training to tensorboard by creating an appropriate writer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 2.2766\n",
      "Epoch: [1/5], Step: [200/600], Loss: 2.2587\n",
      "Epoch: [1/5], Step: [300/600], Loss: 2.2602\n",
      "Epoch: [1/5], Step: [400/600], Loss: 2.2145\n",
      "Epoch: [1/5], Step: [500/600], Loss: 2.1932\n",
      "Epoch: [1/5], Step: [600/600], Loss: 2.1668\n",
      "Epoch: [2/5], Step: [100/600], Loss: 2.1463\n",
      "Epoch: [2/5], Step: [200/600], Loss: 2.0933\n",
      "Epoch: [2/5], Step: [300/600], Loss: 2.0724\n",
      "Epoch: [2/5], Step: [400/600], Loss: 2.0611\n",
      "Epoch: [2/5], Step: [500/600], Loss: 2.0643\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.9820\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.9856\n",
      "Epoch: [3/5], Step: [200/600], Loss: 1.9195\n",
      "Epoch: [3/5], Step: [300/600], Loss: 1.8944\n",
      "Epoch: [3/5], Step: [400/600], Loss: 1.8905\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.8609\n",
      "Epoch: [3/5], Step: [600/600], Loss: 1.8209\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.8037\n",
      "Epoch: [4/5], Step: [200/600], Loss: 1.7698\n",
      "Epoch: [4/5], Step: [300/600], Loss: 1.7343\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.7219\n",
      "Epoch: [4/5], Step: [500/600], Loss: 1.6451\n",
      "Epoch: [4/5], Step: [600/600], Loss: 1.5849\n",
      "Epoch: [5/5], Step: [100/600], Loss: 1.5640\n",
      "Epoch: [5/5], Step: [200/600], Loss: 1.5700\n",
      "Epoch: [5/5], Step: [300/600], Loss: 1.5656\n",
      "Epoch: [5/5], Step: [400/600], Loss: 1.4437\n",
      "Epoch: [5/5], Step: [500/600], Loss: 1.5009\n",
      "Epoch: [5/5], Step: [600/600], Loss: 1.4022\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    loss_tot = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_tot+=loss.detach().data\n",
    "        writer.add_scalar('data/loss',loss.detach().data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss))\n",
    "writer.add_scalar('loss',loss_tot/len(train_dataset), epoch)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images = images.view(-1, 28*28)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "\n",
    "Compare the performance of an MLP with sigmoid activation units against a rectified network.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()#inheritance\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = Net(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradient (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "\n",
    "Very deep networks are known to be affected by the \"Vanishing Gradient\" problem.\n",
    "Investigate this phenomenon by defining an architecture which would suffer from this issue and compare the gradients that you obtain with the ones of a shallower network by using the weight.grad.norm() method.\n",
    "Plot the results.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0120)\n",
      "tensor(0.0134)\n",
      "tensor(0.0128)\n",
      "tensor(0.0130)\n",
      "tensor(0.0122)\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()#inheritance\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.layers = nn.Sequential(nn.ReLU(),nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),nn.Linear(hidden_size, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.layers(out)\n",
    "        return out\n",
    "\n",
    "model = Net(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        print(model.fc1.weight.grad.norm())\n",
    "        break\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                  % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
