{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of lecture5-gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jki1Y1-Jt13K",
        "colab_type": "text"
      },
      "source": [
        "# 5. Generative adversarial networks\n",
        "\n",
        "- Used as part of INFO8010 Deep Learning (Gilles Louppe, 2018-2019).\n",
        "- Originally adapted from [Pytorch tutorial for Deep Learning researchers](https://github.com/yunjey/pytorch-tutorial) (Yunvey Choi, 2018).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kt8qkHLt13L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "#import tensorboardX\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP4mEdypt13P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP7RP4v3uVeH",
        "colab_type": "code",
        "outputId": "ee01c1e4-5c65-431e-e1d3-1fa669a8ef74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3qA8PzEt13V",
        "colab_type": "text"
      },
      "source": [
        "# Hyper-parameters\n",
        "\n",
        "The hyperparameters that are needed for successfully training your first GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmDmap1bt13W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_size = 64\n",
        "hidden_size = 256\n",
        "image_size = 784\n",
        "num_epochs = 30\n",
        "batch_size = 100\n",
        "sample_dir = 'samples'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOdvGhwBt13Z",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "We define a data generator as usual which will normalize the input data before feeding it to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA7BQ8Dht13a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a directory if not exists\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "\n",
        "# Image processing\n",
        "transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5],   # 3 for RGB channels\n",
        "                                     std=[0.5])])\n",
        "\n",
        "# MNIST dataset\n",
        "mnist = torchvision.datasets.MNIST(root='./data/',\n",
        "                                   train=True,\n",
        "                                   transform=transform,\n",
        "                                   download=True)\n",
        "\n",
        "# Data loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZV72C7et13d",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE</b>:\n",
        "\n",
        "Define the Discriminator and Generator neural architectures.\n",
        "Remember that these are the building blocks of any neural architecture, so you should consider:\n",
        "<ul>\n",
        "    <li>The amount of hidden layers and units </li>\n",
        "    <li>Non-linearities between layers</li>\n",
        "    <li>Final activation function (should be tanh for the generator)</li>\n",
        "</ul>\n",
        "   \n",
        "Pay special attention to the last activation function of your models.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHse5XtMt13e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(image_size,512),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(512,256),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(256,hidden_size),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(hidden_size,1),\n",
        "    nn.Sigmoid())\n",
        "# Generator (ending )\n",
        "G = nn.Sequential(\n",
        "    nn.Linear(latent_size,hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size,256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256,512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512,image_size),\n",
        "    nn.Tanh())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Po4FVVt13k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device setting\n",
        "D = D.to(device)\n",
        "G = G.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WwzhxRjt13u",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE</b>:\n",
        "\n",
        "Define the loss function you would like to minimize and two appropriate optimizers with reasonable learning rates.\n",
        "Remember what has been discussed during the lecture.\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlHfGC70t13v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.SGD(D.parameters(), lr=0.001, momentum=0.9)\n",
        "g_optimizer = torch.optim.SGD(G.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQXoYssxt13z",
        "colab_type": "text"
      },
      "source": [
        "# Utils\n",
        "\n",
        "Here are some auxiliary functions which will make your training easier.\n",
        "<ul>\n",
        "    <li>The first function transforms the output of your generator into a meaningful image.</li>\n",
        "    <li>The second function will avoid the accumulation of the gradients during training: you will have to call this function yourself!</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sao3EhAat131",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utils\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)\n",
        "\n",
        "def reset_grad():\n",
        "    d_optimizer.zero_grad()\n",
        "    g_optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT-0hnS6t134",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE</b>:\n",
        "\n",
        "\n",
        "Here are the steps you need to define if you want a proper training loop:\n",
        "\n",
        "<ul>\n",
        "    <li>Start by defining the labels that are used at training time </li>\n",
        "    <li>Evaluate the performance of the discriminator based on the different inputs that it requires:\n",
        "    <ul>\n",
        "        <li> Remember that it has access to two things, therefore be sure to use them both and to compute its final loss accordingly </li>\n",
        "    </ul>\n",
        "    </li>\n",
        "   <li> Make a proper use of the tensorboard methods that are being used, what do they expect as input? </li>\n",
        "    <li> Be sure the make a good use of the utils </li>\n",
        "    \n",
        "</ul>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnwcbIDPt135",
        "colab_type": "code",
        "outputId": "a2345343-e4c1-4273-a776-9343f0bf0984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_step = len(data_loader)\n",
        "\n",
        "# Add appropriate labels\n",
        "ten = torch.ones(batch_size,1).to(device)\n",
        "fake_labels = torch.zeros(batch_size,1).to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(data_loader):\n",
        "        images = images.reshape(batch_size, -1).to(device)\n",
        "        \n",
        "        ## Train your discriminator\n",
        "\n",
        "        outputs = D(images)\n",
        "        \n",
        "        d_loss_real = criterion(outputs,ten)#firstly, we compare true (100%) and true of outputs(softmax)\n",
        "        \n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        d_loss_fake = criterion(outputs,fake_labels)\n",
        "        \n",
        "        # Backprop and optimize the discriminator\n",
        "        d_loss = d_loss_real+d_loss_fake\n",
        "        reset_grad()\n",
        "        d_loss.backward(retain_graph=True)\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        ## Train your generator\n",
        "        \n",
        "        outputs = D(fake_images)        \n",
        "        g_loss = -criterion(outputs, fake_labels) #compare zeros\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        reset_grad()\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        g_optimizer.step()\n",
        "        \n",
        "        if (i+1) % 200 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}' \n",
        "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item()))\n",
        "    \n",
        "    # Save real images\n",
        "    if (epoch+1) == 1:\n",
        "        images = images.reshape(images.size(0), 1, 28, 28)\n",
        "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
        "        \n",
        "    # Save sampled images\n",
        "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
        "    \n",
        "# Save the model checkpoints \n",
        "torch.save(G.state_dict(), 'G.ckpt')\n",
        "torch.save(D.state_dict(), 'D.ckpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/30], Step [200/600], d_loss: 0.3778, g_loss: -0.3711\n",
            "Epoch [0/30], Step [400/600], d_loss: 0.1601, g_loss: -0.1579\n",
            "Epoch [0/30], Step [600/600], d_loss: 0.0760, g_loss: -0.0747\n",
            "Epoch [1/30], Step [200/600], d_loss: 0.0419, g_loss: -0.0412\n",
            "Epoch [1/30], Step [400/600], d_loss: 0.0262, g_loss: -0.0258\n",
            "Epoch [1/30], Step [600/600], d_loss: 0.0181, g_loss: -0.0177\n",
            "Epoch [2/30], Step [200/600], d_loss: 0.0132, g_loss: -0.0130\n",
            "Epoch [2/30], Step [400/600], d_loss: 0.0102, g_loss: -0.0100\n",
            "Epoch [2/30], Step [600/600], d_loss: 0.0081, g_loss: -0.0080\n",
            "Epoch [3/30], Step [200/600], d_loss: 0.0067, g_loss: -0.0066\n",
            "Epoch [3/30], Step [400/600], d_loss: 0.0057, g_loss: -0.0056\n",
            "Epoch [3/30], Step [600/600], d_loss: 0.0048, g_loss: -0.0047\n",
            "Epoch [4/30], Step [200/600], d_loss: 0.0042, g_loss: -0.0041\n",
            "Epoch [4/30], Step [400/600], d_loss: 0.0037, g_loss: -0.0036\n",
            "Epoch [4/30], Step [600/600], d_loss: 0.0034, g_loss: -0.0032\n",
            "Epoch [5/30], Step [200/600], d_loss: 0.0029, g_loss: -0.0029\n",
            "Epoch [5/30], Step [400/600], d_loss: 0.0027, g_loss: -0.0026\n",
            "Epoch [5/30], Step [600/600], d_loss: 0.0024, g_loss: -0.0024\n",
            "Epoch [6/30], Step [200/600], d_loss: 0.0022, g_loss: -0.0022\n",
            "Epoch [6/30], Step [400/600], d_loss: 0.0020, g_loss: -0.0020\n",
            "Epoch [6/30], Step [600/600], d_loss: 0.0019, g_loss: -0.0018\n",
            "Epoch [7/30], Step [200/600], d_loss: 0.0017, g_loss: -0.0017\n",
            "Epoch [7/30], Step [400/600], d_loss: 0.0016, g_loss: -0.0016\n",
            "Epoch [7/30], Step [600/600], d_loss: 0.0015, g_loss: -0.0015\n",
            "Epoch [8/30], Step [200/600], d_loss: 0.0014, g_loss: -0.0014\n",
            "Epoch [8/30], Step [400/600], d_loss: 0.0014, g_loss: -0.0013\n",
            "Epoch [8/30], Step [600/600], d_loss: 0.0013, g_loss: -0.0012\n",
            "Epoch [9/30], Step [200/600], d_loss: 0.0012, g_loss: -0.0012\n",
            "Epoch [9/30], Step [400/600], d_loss: 0.0011, g_loss: -0.0011\n",
            "Epoch [9/30], Step [600/600], d_loss: 0.0011, g_loss: -0.0011\n",
            "Epoch [10/30], Step [200/600], d_loss: 0.0010, g_loss: -0.0010\n",
            "Epoch [10/30], Step [400/600], d_loss: 0.0010, g_loss: -0.0010\n",
            "Epoch [10/30], Step [600/600], d_loss: 0.0009, g_loss: -0.0009\n",
            "Epoch [11/30], Step [200/600], d_loss: 0.0009, g_loss: -0.0009\n",
            "Epoch [11/30], Step [400/600], d_loss: 0.0009, g_loss: -0.0008\n",
            "Epoch [11/30], Step [600/600], d_loss: 0.0008, g_loss: -0.0008\n",
            "Epoch [12/30], Step [200/600], d_loss: 0.0008, g_loss: -0.0008\n",
            "Epoch [12/30], Step [400/600], d_loss: 0.0008, g_loss: -0.0007\n",
            "Epoch [12/30], Step [600/600], d_loss: 0.0007, g_loss: -0.0007\n",
            "Epoch [13/30], Step [200/600], d_loss: 0.0007, g_loss: -0.0007\n",
            "Epoch [13/30], Step [400/600], d_loss: 0.0007, g_loss: -0.0007\n",
            "Epoch [13/30], Step [600/600], d_loss: 0.0007, g_loss: -0.0006\n",
            "Epoch [14/30], Step [200/600], d_loss: 0.0006, g_loss: -0.0006\n",
            "Epoch [14/30], Step [400/600], d_loss: 0.0006, g_loss: -0.0006\n",
            "Epoch [14/30], Step [600/600], d_loss: 0.0006, g_loss: -0.0006\n",
            "Epoch [15/30], Step [200/600], d_loss: 0.0006, g_loss: -0.0006\n",
            "Epoch [15/30], Step [400/600], d_loss: 0.0006, g_loss: -0.0005\n",
            "Epoch [15/30], Step [600/600], d_loss: 0.0005, g_loss: -0.0005\n",
            "Epoch [16/30], Step [200/600], d_loss: 0.0005, g_loss: -0.0005\n",
            "Epoch [16/30], Step [400/600], d_loss: 0.0005, g_loss: -0.0005\n",
            "Epoch [16/30], Step [600/600], d_loss: 0.0005, g_loss: -0.0005\n",
            "Epoch [17/30], Step [200/600], d_loss: 0.0005, g_loss: -0.0005\n",
            "Epoch [17/30], Step [400/600], d_loss: 0.0005, g_loss: -0.0005\n",
            "Epoch [17/30], Step [600/600], d_loss: 0.0005, g_loss: -0.0004\n",
            "Epoch [18/30], Step [200/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [18/30], Step [400/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [18/30], Step [600/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [19/30], Step [200/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [19/30], Step [400/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [19/30], Step [600/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [20/30], Step [200/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [20/30], Step [400/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [20/30], Step [600/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [21/30], Step [200/600], d_loss: 0.0004, g_loss: -0.0004\n",
            "Epoch [21/30], Step [400/600], d_loss: 0.0003, g_loss: -0.0003\n",
            "Epoch [21/30], Step [600/600], d_loss: 0.0003, g_loss: -0.0003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c57ad698c64c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-jCiKKht138",
        "colab_type": "text"
      },
      "source": [
        "# When you think you are done ...\n",
        "\n",
        "Training this model on your own machine can be particularly expensive if you do not have a GPU.\n",
        "Therefore you can try to run your notebook on [Google-Colab](https://colab.research.google.com/notebooks/welcome.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm3VXW67t139",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE</b>:\n",
        "\n",
        "\n",
        "Investigate what happens when using always the same set of images within a training batch.\n",
        "Monitor the behaviour of the loss and the resulting generated samples in tensorboard.\n",
        "Which phenomenon do you observe?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXVT7sYlt139",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE</b>:\n",
        "    \n",
        "Now instead of using the MNIST dataset try to train a GAN on the images coming from the [Cifar-10]()\n",
        "dataset. In this case you should define an architecture based on convolutional layers.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDRpPiAVt13-",
        "colab_type": "code",
        "outputId": "ef4f9d01-5c7b-4969-c64a-0c55f877c244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "z = torch.randn(latent_size).to(device)\n",
        "fake_image = G(z)\n",
        "fake_image = denorm(fake_image)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "fake_image = np.array(fake_image.tolist()).reshape(28, 28)\n",
        "plt.imshow(fake_image,'Dark2')\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "fake_image = np.array(fake_image.tolist()).reshape(28,28)\n",
        "new = Image.fromarray(fake_image)\"\"\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport numpy as np\\nfrom PIL import Image\\nfake_image = np.array(fake_image.tolist()).reshape(28,28)\\nnew = Image.fromarray(fake_image)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngvpfXEpvUIuTg0UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX0hcM6Nt14A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}